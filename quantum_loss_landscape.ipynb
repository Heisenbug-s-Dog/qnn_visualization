{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"quantum_loss_landscape.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNVHQMEhVhcHWJMpXZ9sAMJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"tkP_5Hkc9Bxm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613650025711,"user_tz":-540,"elapsed":18692,"user":{"displayName":"JOON SUK HUH","photoUrl":"","userId":"03235780383723321017"}},"outputId":"01ce2535-d1d6-473f-9752-3bfb6d71ae46"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pz_MKrmnDPFC"},"source":["!pip install torch==1.3.1\r\n","!pip install torchvision==0.4.2\r\n","!pip install Pillow==6.2.1\r\n","!pip install pennylane==0.7.0\r\n","!pip install loss-landscapes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09ha3wQi9GJw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613650245571,"user_tz":-540,"elapsed":1619,"user":{"displayName":"JOON SUK HUH","photoUrl":"","userId":"03235780383723321017"}},"outputId":"ce0267f3-dce0-44b5-a3ce-683e8d585b67"},"source":["# OpenMP: number of parallel threads.\r\n","%env OMP_NUM_THREADS=1\r\n","\r\n","# Plotting\r\n","%matplotlib inline\r\n","import matplotlib.pyplot as plt\r\n","\r\n","# PyTorch\r\n","import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","from torch.optim import lr_scheduler\r\n","import torchvision\r\n","from torchvision import datasets, models, transforms\r\n","\r\n","# Pennylane\r\n","import pennylane as qml\r\n","from pennylane import numpy as np\r\n","\r\n","# Other tools\r\n","import time\r\n","import copy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["env: OMP_NUM_THREADS=1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vB31Zr4O9LLZ"},"source":["filtered_classes = ['cat', 'dog']  # Subset of CIFAR ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n","n_qubits = 4                       # Number of qubits\r\n","quantum = False                     # If set to \"False\", the dressed quantum circuit is replaced by \r\n","                                   # An enterily classical net (defined by the next parameter). \r\n","classical_model = '512_nq_n'          # Possible choices: '512_n','512_nq_n','551_512_n'. [nq=n_qubits, n=num_filtered_classes]\r\n","step = 0.001                       # Learning rate\r\n","batch_size = 8                     # Number of samples for each training step\r\n","num_epochs = 3                     # Number of training epochs\r\n","q_depth = 5                        # Depth of the quantum circuit (number of variational layers)\r\n","gamma_lr_scheduler = 1             # Learning rate reduction applied every 10 epochs.                       \r\n","max_layers = 15                    # Keep 15 even if not all are used.\r\n","q_delta = 0.01                     # Initial spread of random quantum weights\r\n","rng_seed = 0                       # Seed for random number generator\r\n","start_time = time.time()           # start of the computation timer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOlFcz8O9MrU"},"source":["torch.manual_seed(rng_seed)\r\n","dev = qml.device('default.qubit', wires=n_qubits)\r\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n","#device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8at2cnC9SIs"},"source":["def H_layer(nqubits):\r\n","    \"\"\"Layer of single-qubit Hadamard gates. \r\n","    \"\"\"\r\n","    for idx in range(nqubits):\r\n","        qml.Hadamard(wires=idx)\r\n","        \r\n","def RY_layer(w):\r\n","    \"\"\"Layer of parametrized qubit rotations around the y axis. \r\n","    \"\"\"\r\n","    for idx, element in enumerate(w):\r\n","        qml.RY(element, wires=idx)\r\n","\r\n","def entangling_layer(nqubits):\r\n","    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\r\n","    \"\"\"\r\n","    # In other words it should apply something like :\r\n","    # CNOT  CNOT  CNOT  CNOT...  CNOT\r\n","    #   CNOT  CNOT  CNOT...  CNOT  \r\n","    for i in range(0, nqubits - 1, 2): # Loop over even indices: i=0,2,...N-2  \r\n","        qml.CNOT(wires=[i, i + 1])\r\n","    for i in range(1, nqubits - 1,2): # Loop over odd indices:  i=1,3,...N-3\r\n","        qml.CNOT(wires=[i, i + 1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPin1xjw9cmP"},"source":["@qml.qnode(dev, interface='torch')\r\n","def q_net(q_in, q_weights_flat):\r\n","        \r\n","        # Reshape weights\r\n","        q_weights = q_weights_flat.reshape(max_layers, n_qubits)\r\n","        \r\n","        # Start from state |+> , unbiased w.r.t. |0> and |1>\r\n","        H_layer(n_qubits)\r\n","        \r\n","        # Embed features in the quantum node\r\n","        RY_layer(q_in)\r\n","       \r\n","        # Sequence of trainable variational layers\r\n","        for k in range(q_depth):\r\n","            entangling_layer(n_qubits)\r\n","            RY_layer(q_weights[k+1])\r\n","\r\n","        # Expectation values in the Z basis\r\n","        return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqQvKZ6P9eTb"},"source":["class Quantumnet(nn.Module):\r\n","        def __init__(self):\r\n","            super().__init__()\r\n","            self.pre_net = nn.Linear(512, n_qubits)\r\n","            self.q_params = nn.Parameter(q_delta * torch.randn(max_layers * n_qubits))\r\n","            self.post_net = nn.Linear(n_qubits, len(filtered_classes))\r\n","\r\n","        def forward(self, input_features):\r\n","            pre_out = self.pre_net(input_features) \r\n","            q_in = torch.tanh(pre_out) * np.pi / 2.0   \r\n","            \r\n","            # Apply the quantum circuit to each element of the batch, and append to q_out\r\n","            q_out = torch.Tensor(0, n_qubits)\r\n","            q_out = q_out.to(device)\r\n","            for elem in q_in:\r\n","                q_out_elem = q_net(elem,self.q_params).float().unsqueeze(0)\r\n","                q_out = torch.cat((q_out, q_out_elem))\r\n","            return self.post_net(q_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AH41Fm8l9iYK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613650295703,"user_tz":-540,"elapsed":3594,"user":{"displayName":"JOON SUK HUH","photoUrl":"","userId":"03235780383723321017"}},"outputId":"1fc5480d-303b-4194-a846-b171b5f4253d"},"source":["# Build Model\r\n","model_hybrid = torchvision.models.resnet18(pretrained=True)\r\n","\r\n","for param in model_hybrid.parameters():\r\n","    param.requires_grad = False\r\n","\r\n","if quantum:\r\n","    model_hybrid.fc = Quantumnet()\r\n","\r\n","elif classical_model == '512_n':\r\n","    model_hybrid.fc = nn.Linear(512,len(filtered_classes))\r\n","\r\n","elif classical_model == '512_nq_n':\r\n","    model_hybrid.fc = nn.Sequential(nn.Linear(512, n_qubits),torch.nn.ReLU(),nn.Linear(n_qubits, len(filtered_classes))) \r\n","\r\n","elif classical_model == '551_512_n':\r\n","    model_hybrid.fc = nn.Sequential(nn.Linear(512, 512), torch.nn.ReLU(), nn.Linear(512, len(filtered_classes)))\r\n","\r\n","# Use CUDA or CPU according to the \"device\" object.\r\n","model_hybrid = model_hybrid.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 237MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YqWRE_W09i3K"},"source":["# Load model from file\r\n","path = '/content/drive/MyDrive/Qiskit-Hackathon-Korea/qnn-visualization/'\r\n","\r\n","if quantum:\r\n","    model_hybrid.load_state_dict(torch.load(\r\n","        path+'quantum_' + filtered_classes[0] + '_' + filtered_classes[1] + '.pt'\r\n","        )\r\n","    )\r\n","                                 \r\n","else:\r\n","    model_hybrid.load_state_dict(torch.load(\r\n","        path+'classical_' + filtered_classes[0] + '_' + filtered_classes[1] + '.pt'\r\n","        )\r\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTX4NIHsKpz4","executionInfo":{"status":"ok","timestamp":1613650301623,"user_tz":-540,"elapsed":9223,"user":{"displayName":"JOON SUK HUH","photoUrl":"","userId":"03235780383723321017"}},"outputId":"525909ce-556e-4f17-b4ba-fc78763fb0a8"},"source":["# Fixed pre-processing operations\r\n","data_transforms = {\r\n","    'train': transforms.Compose([\r\n","        #transforms.RandomResizedCrop(224),     # uncomment for data augmentation\r\n","        #transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\r\n","        transforms.Resize(256),\r\n","        transforms.CenterCrop(224),\r\n","        transforms.ToTensor(),\r\n","        # Normalize input channels using mean values and standard deviations of ImageNet.\r\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n","    ]),\r\n","    'val': transforms.Compose([\r\n","        transforms.Resize(256),\r\n","        transforms.CenterCrop(224),\r\n","        transforms.ToTensor(),\r\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n","    ]),\r\n","}\r\n","\r\n","\r\n","# =================== begin CIFAR dataset loading ===================\r\n","trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,\r\n","                                        download=True, transform=data_transforms['train'])\r\n","testset_full = torchvision.datasets.CIFAR10(root='./data', train=False,\r\n","                                       download=True, transform=data_transforms['val'])\r\n","image_datasets_full={'train': trainset_full, 'val': testset_full}\r\n","\r\n","# CIFAR classes\r\n","class_names = ('plane', 'car', 'bird', 'cat',\r\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n","\r\n","# Get indices of samples associated to filtered_classes\r\n","filtered_labels=[class_names.index(cl) for cl in filtered_classes]\r\n","sub_indices={'train': [], 'val': []}\r\n","for phase in ['train', 'val']:\r\n","    for idx, label in enumerate(image_datasets_full[phase].targets):  \r\n","        if label in filtered_labels:\r\n","            sub_indices[phase].append(idx)\r\n","            \r\n","# Initialize sub-datasets according to filtered indices\r\n","image_datasets = {x: torch.utils.data.Subset(image_datasets_full[x], sub_indices[x])\r\n","                for x in ['train', 'val']}\r\n","\r\n","def labels_to_filtered(labels):\r\n","    \"\"\"Maps CIFAR labels (0,1,2,3,4,5,6,7,8,9) to the index of filtered_labels\"\"\"\r\n","    return [filtered_labels.index(label) for label in labels]\r\n","# =================== end CIFAR dataset loading ==========================\r\n","\r\n","# Number of samples\r\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\r\n","\r\n","# Initialize dataloader\r\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \r\n","                  batch_size=batch_size, shuffle=True, num_workers=0) for x in ['train', 'val']}\r\n","\r\n","# Function to plot images from tensors\r\n","def imshow(inp, title=None):\r\n","    \"\"\"Imshow for Tensor.\"\"\"\r\n","    inp = inp.numpy().transpose((1, 2, 0))\r\n","    # We apply the inverse of the initial normalization operation.\r\n","    mean = np.array([0.485, 0.456, 0.406])\r\n","    std = np.array([0.229, 0.224, 0.225])\r\n","    inp = std * inp + mean\r\n","    inp = np.clip(inp, 0, 1)\r\n","    plt.imshow(inp)\r\n","    if title is not None:\r\n","        plt.title(title)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 8192/170498071 [00:00<37:46, 75217.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["170500096it [00:01, 92080669.55it/s]                               \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MPzUasUkXXj3"},"source":["from sklearn.manifold import TSNE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPsV8l8KosYr"},"source":["# Get a batch of training data\r\n","X, y_cifar = next(iter(dataloaders['train']))\r\n","y = torch.tensor(labels_to_filtered(y_cifar))\r\n","X, y = X.to(device), y.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"lP5yK69hnNby","executionInfo":{"status":"error","timestamp":1613650304370,"user_tz":-540,"elapsed":11274,"user":{"displayName":"JOON SUK HUH","photoUrl":"","userId":"03235780383723321017"}},"outputId":"81cfa9c5-1d19-475a-9d23-f34ea37f718c"},"source":["loss_func = nn.CrossEntropyLoss()\r\n","#loss_func = nn.MSELoss(reduce='false')\r\n","metric = Loss(loss_func, X, y)\r\n","landscape = random_plane(model_hybrid, metric, normalization='filter')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-fd201f6a7617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#loss_func = nn.MSELoss(reduce='false')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlandscape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_plane\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_hybrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'filter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/loss_landscapes/main.py\u001b[0m in \u001b[0;36mrandom_plane\u001b[0;34m(model, metric, distance, steps, normalization, deepcopy_model)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mdir_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mdir_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mstart_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0mstart_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_two\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mdir_one\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruediv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/loss_landscapes/model_interface/model_parameters.py\u001b[0m in \u001b[0;36msub_\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'ModelParameters'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"]}]},{"cell_type":"code","metadata":{"id":"x0XNhO1VtaTX"},"source":[""],"execution_count":null,"outputs":[]}]}